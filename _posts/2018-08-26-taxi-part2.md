---
title: "Google BigQuery Taxi Fare Predictions - Part 2"
date: 2018-08-26
tags: [regression, random forest, xgboost]
header:
  image: "/images/taxi_pic.jpg"
excerpt: "Regression, Random Forest, XGBoost"
mathjax: "true"
---

# Model preparation

On this stage we are working on regression problem to build a predictive model for taxi fares.<br>
To derive percise predictions we will take different algorithms into consideration:

1. Multiple regresssion with regularization component(L1/L2);
2. Random forest model;
3. XGBoost model.

### 1.1 Linear model without regularization

We start with spliting data on train-validation and test. Using 10% of data as test sample we train basic linear model and estimate performance metrics - MAE (mean absolute error) and RMSE (root squared error) on test. 
To simplify our code on further steps we will write a function to estimate metrics for different models. 
<details><summary>Python code</summary> 
  
 ```python
"""
- perform train_test_split() mudule for data split
- separate the  relevant features list and target as an output 

"""
train_valid_data, test_data = train_test_split(data, test_size=0.1)

features=['dropoff_longitude','dropoff_latitude', 'pickup_longitude', 'pickup_latitude', 
         'distance_trip', 'diff', 'passenger_count', 'dropoff_month', 'dropoff_day', 'dropoff_hour',
         'pickup_month', 'pickup_day', 'pickup_hour']
output='fare_amount'

"""
- define the function to compute errors 
- print the result on test data

"""

def compute_error(predictions, true_values):
    resid=true_values-predictions
    rss = sum(resid*resid)
    # computing root mean absolute error 
    mae_true=sum(abs(resid))/len(predictions)
    # computing root mean squared error 
    mse_true=rss/len(resid)
    rmse_true=np.sqrt(mse_true)
    
    print "RMSE on test equals "+ "{:.{}f}".format( rmse_true, 2 )
    print "MAE on test  equals "+"{:.{}f}".format( mae_true, 2 )  

 ```
</details>


Now we are ready to train base regression model on *train_valid_data* and evaluate performace on test. 

<details><summary>Python code</summary> 
  
<p>
  
 ```python
"""
- create a linear regression object
- fit with train-validation data
- make predictions on test

"""
linear=LinearRegression(normalize=True)
linear.fit(train_valid_data[features], train_valid_data[output])
predictions=linear.predict(test_data[features])
compute_error(predictions, test_data[output])

 ```
 </p>
</details>



![LSTM]({{ 'taxi_output/base_model.PNG' | absolute_url }})

The results above represent performance for unregularized model. Let's check the possible improvement with *regularization parameter added.*
